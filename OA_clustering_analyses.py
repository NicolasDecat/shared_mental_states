#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Sep 19 14:07:59 2023NapNest: Subjective dimensions clusteringPCA, elbow method and k-means clusteringFollowed by analyses on the clusters and their EEG correlates@author: nico"""import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.decomposition import PCAfrom sklearn.metrics import silhouette_scorefrom sklearn.cluster import KMeansfrom matplotlib.lines import Line2Dimport matplotlib#%% Import data ################################################################ Import datacsv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable.csv'table = pd.read_csv(csv_path, usecols= range(0, 18))dimensions = ['Bizarreness', 'Fluidity', 'Spontaneity','Wake level']#%% Exclude data ################################################################# subs with behavioral issuesbehavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()# subs who had no content (0 in all dimensions)subs_no_content = table.index[table[dimensions[:-1]].sum(axis=1) == 0].tolist()# subs with 1 NaN (0 in 1 of the dimensions)subs_nan = table.index[table[dimensions].isnull().any(axis=1)].tolist()# All subs to excludesubs_to_excl = list(set(sorted(sub_behav + subs_no_content + subs_nan)))# Remove the excluded substable_subs_excluded = table.drop(subs_to_excl)scores = table[dimensions].drop(subs_to_excl)num_unique_subjects = table_subs_excluded["Subject"].nunique()# Compute number of trials per subjectrepetitions_per_subject = table_subs_excluded['Subject'].value_counts()mean_repetitions = repetitions_per_subject.mean()std_repetitions = repetitions_per_subject.std()#%% PCA ######################################################################## PCAn_components = 3pca = PCA(n_components=n_components)pca_result = pca.fit_transform(scores)pca_df = pd.DataFrame(data=pca_result, columns=[f'PC{i}' for i in range(1, n_components + 1)])# Explained varianceexpl_variance = pca.explained_variance_ratio_cum_expl_variance = np.cumsum(expl_variance)print("Explained Variance :", expl_variance)# Plot the PC profiles (loadings heatmap)fig = plt.figure(figsize=(10, 8))loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i}' for i in range(1, n_components + 1)], index=scores.columns)loadings_df = loadings.transpose()sns.heatmap(loadings_df, cmap="coolwarm", annot=True, fmt=".2f", linewidths=.5)plt.xlabel("Dimensions")plt.ylabel("PCs")plt.tight_layout()plt.show()# Plot the explained variance (Scree plot)fig = plt.figure(figsize=(10, 8))plt.rcParams.update({'font.size': 30})plt.plot(range(1, len(cum_expl_variance) + 1), cum_expl_variance, marker='o', linestyle='-', color='k',linewidth=2.5)plt.xlabel('Principal components')plt.ylabel('Cumulative\nexplained variance (%)')plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], ['0','20', '40', '60', '80', '100'])plt.xticks(range(1, len(cum_expl_variance) + 1))plt.grid(axis='y')plt.gca().spines['right'].set_visible(False)plt.gca().spines['top'].set_visible(False)plt.gcf().set_facecolor('white')plt.tight_layout()plt.show()# plt.savefig('/Users/nico/Documents/PhD/conferences/ASSC2024/poster/explained_variance.png', dpi=300, bbox_inches='tight')# TABLE UPDATE: Add the PCA results (except for excluded subs)table.loc[~table.index.isin(subs_to_excl), [f'PC{i}' for i in range(1, n_components + 1)]] = pca_df.values #%% Elbow method ############################################################### Set parametersinertia_values = []silhouette_scores = []max_clusters = 10  # Compute the silhouette scoresfor i in range(2, max_clusters + 1):  # Silhouette score requires at least 2 clusters    kmeans = KMeans(n_clusters=i, random_state=42)    kmeans.fit(pca_df)    inertia_values.append(kmeans.inertia_)    silhouette_scores.append(silhouette_score(pca_df, kmeans.labels_))    # Plotting the Elbow Method graph with Silhouette Scoreplt.rcParams.update({'font.size': 25})fig, ax1 = plt.subplots(figsize=(10, 8))color = 'tab:green'ax1.set_xlabel('Number of Clusters')ax1.set_ylabel('Inertia', color=color)line1, = ax1.plot(range(1, max_clusters), inertia_values, marker='o', color=color)  # Start from 2ax1.tick_params(axis='y', labelcolor=color)ax2 = ax1.twinx()color = 'tab:blue'ax2.set_ylabel('Silhouette Score', color=color)line2, = ax2.plot(range(1, max_clusters), silhouette_scores, marker='o', color=color)  # Start from 2ax2.tick_params(axis='y', labelcolor=color)plt.title('Elbow Method with Silhouette Score')plt.xticks(range(1, max_clusters))  # Start from 2plt.tight_layout()plt.show()# paper figfig, ax1 = plt.subplots(figsize=(10, 8))font_size = 30ax1.set_xlabel('Number of clusters', fontsize=font_size)ax1.set_ylabel('Inertia', fontsize=font_size)line1, = ax1.plot(range(1, max_clusters), inertia_values, marker='o', linestyle='-', color='k', linewidth=3)  # Start from 1ax1.tick_params(axis='y', labelsize=font_size)plt.xticks(range(1, max_clusters), fontsize=font_size)ax1.spines['right'].set_visible(False)ax1.spines['top'].set_visible(False)plt.grid(axis='y')plt.tight_layout()plt.show()# plt.savefig('/Users/nico/Documents/PhD/conferences/ASSC2024/poster/elbow.png', dpi=300, bbox_inches='tight')#%% k-means clustering on PCA space ############################################ Parametersoptimal_clusters = 4kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)cluster_labels = kmeans.fit_predict(pca_df)centroids = pd.DataFrame(kmeans.cluster_centers_, columns=[f'C{i}' for i in range(1, n_components + 1)])cluster_labels += 1 # To get clust 1,2,3scores['Cluster'] = cluster_labels# Plot clusters (3D)cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}plt.rcParams.update({'font.size': 28})fig = plt.figure(figsize=(10, 8))ax = fig.add_subplot(111, projection='3d')  scatter = ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'],                      c=[cluster_colors[label] for label in cluster_labels],                      cmap='viridis', linewidth=0.2, s=100, alpha=0.8)ax.scatter(centroids.iloc[:, 0], centroids.iloc[:, 1], centroids.iloc[:, 2], marker='X', color='black', s=100, label='Centroids')ax.set_xlabel('PC1')ax.set_ylabel('PC2')ax.set_zlabel('PC3')ax.set_xlim([pca_df['PC1'].min(), pca_df['PC1'].max()])ax.set_ylim([pca_df['PC2'].min(), pca_df['PC2'].max()])ax.set_zlim([pca_df['PC3'].min(), pca_df['PC3'].max()])ax.set_xticklabels(ax.set_yticklabels(ax.set_zticklabels([])))plt.show()# plt.savefig('kmeans.png', dpi=300, bbox_inches='tight')# TABLE UPDATE: Add the cluster labels table.loc[~table.index.isin(subs_to_excl), ['Cluster']] = cluster_labelstable_subs_excluded.loc[~table_subs_excluded.index.isin(subs_to_excl), ['Cluster']] = cluster_labels# table.to_csv('/Users/nico/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv', index=False)  # Compute proba of belonging to one of the clustersdistances = kmeans.transform(pca_df)  probabilities = np.exp(-distances) / np.sum(np.exp(-distances), axis=1, keepdims=True)# Create a new table, storing sub info and those probatable_proba = table_subs_excluded.iloc[:, [1, 2, 3]].copy()table_proba[['proba1', 'proba2', 'proba3','proba4']] = probabilities# table_proba.to_csv("/Users/nico/Documents/PhD/projects/NapNest/subjective_dimensions/table_proba.csv", index=False)# Plot cluster proba across stages within each clustertable_proba = table_subs_excluded.iloc[:, [1, 2, 3,8,18]].copy()table_proba[['proba1', 'proba2', 'proba3','proba4']] = probabilitiestable_proba['Stage-at-probe'] = table_proba['Stage-at-probe'].replace(['MSE', '1'], 'N1MSE')# Set up the figure with 4 subplots (1 row, 4 columns)fig, axes = plt.subplots(1, 4, figsize=(20, 5))# Map clusters to their corresponding 'proba' columncluster_to_proba = {    1: 'proba1',    2: 'proba2',    3: 'proba3',    4: 'proba4'}#%% Plot subjective properties of each cluster# Calculate the mean of each dimension for each clustercluster_dimension_means = scores.groupby('Cluster').mean()dimension_colors = {    'Bizarreness': '#003333',    'Fluidity':    '#336633',    'Spontaneity': '#99cc33',    'Wake level':  '#c9ea88'}cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# Create a bar plot for each cluster's average dimensionsfig, ax = plt.subplots(figsize=(14, 8))bar_width = 0.45spacing = 1  # Create positions for each bar in the clusterspositions = np.arange( len(cluster_dimension_means)) * (bar_width * len(cluster_dimension_means.columns) + spacing)# Plot barsfor i, col in enumerate(cluster_dimension_means.columns):    ax.bar(positions + i * bar_width, cluster_dimension_means[col], bar_width, label=col, color=dimension_colors[col])# Customize plot aestheticsfontsize = 25ax.set_ylabel('Average Scores', fontsize=fontsize)ax.set_ylim(0, 6)ax.tick_params(axis='y', labelsize=fontsize)cluster_labels = ['Cluster {}'.format(i) for i in cluster_dimension_means.index]ax.set_xticks(positions + bar_width * (len(cluster_dimension_means.columns) - 1) / 2)ax.set_xticklabels(cluster_labels, rotation=0, fontsize=fontsize)legend_labels = dimensionsax.legend(legend_labels, title='Dimension', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=fontsize)ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)plt.axhline(y=3, color='black', linestyle='--', linewidth=1, label='Threshold')plt.rcParams.update({'font.size': fontsize})ax.set_xlabel('')ax.yaxis.grid(True)# Add colored rectangles behind x-tick labels to indicate cluster colorsfor tick_label, color in zip(ax.get_xticklabels(), [cluster_colors[i] for i in cluster_dimension_means.index]):    tick_label.set_bbox(dict(facecolor=color, alpha=0.5, edgecolor='none'))for bar in ax.patches:    bar.set_zorder(2)plt.tight_layout()plt.show()# plt.savefig('/Users/nico/Documents/PhD/conferences/ASSC2024/poster/profils.png', dpi=300, bbox_inches='tight')################################################################################## CLUSTERS DESCRIPTION  #####################################################################################################################################%% Distribution of clusters within each sub ##################################import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# Load table (clusters are included)csv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path)cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# subs with behavioral issues are removedbehavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)# Get the count and percentage of clusters within each subgrouped_by_cluster_count = scores.groupby(['Subject', 'Cluster']).size().unstack(fill_value=0).iloc[:, ::-1]# Only take subs who had at least 2 trialsvalid_subjects = grouped_by_cluster_count.sum(axis=1) >= 2filtered_grouped_by_cluster_count = grouped_by_cluster_count[valid_subjects]grouped_by_cluster_perc = filtered_grouped_by_cluster_count.div(    filtered_grouped_by_cluster_count.sum(axis=1), axis=0) * 100# What's the count / % for each clustercluster_sums = grouped_by_cluster_count.sum(axis=0)total = cluster_sums.sum()cluster_percentages = (cluster_sums / total) * 100# Plot stacked bar plotax = grouped_by_cluster_perc.plot(kind='bar', stacked=True, color=cluster_colors, figsize=(16, 6))ax.tick_params(axis='x', labelsize=12)  # Adjust the number to your preferred font sizeax.set_xlabel('Subject')ax.set_ylabel('Percentage')ax.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')legend_labels = [    f"{cluster} ({count}, {percentage:.1f}%)"    for cluster, count, percentage in zip(cluster_sums.index, cluster_sums, cluster_percentages)]# ax.legend(labels=legend_labels, title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')plt.show()plt.tight_layout()#%% Distribution of Perception across clusters# Define custom colors with float keys as shown in the error messagecluster_colors = {'1.0': '#ff5733', '2.0': '#ffcc00', '3.0': '#3366ff', '4.0': '#cc33ff'}# Filter out NaN values from the Cluster columnfiltered_table = table.dropna(subset=['Cluster'])# Create a figure with a specific sizeplt.figure(figsize=(10, 6))# Create the boxplot with custom colorsax = sns.boxplot(x='Cluster', y='Perception', data=filtered_table,                  palette=cluster_colors, width=0.5, fliersize=0)# Add the individual data points with jitter and transparency# Use the same color scheme for the pointsfor cluster, color in cluster_colors.items():    # Convert cluster key to float for comparison    cluster_value = float(cluster)    cluster_data = filtered_table[filtered_table['Cluster'] == cluster_value]    sns.stripplot(x='Cluster', y='Perception', data=cluster_data,                  size=4, alpha=0.3, jitter=True, color=color,                   edgecolor='gray', linewidth=0.5, ax=ax)# Customize the plotplt.xlabel('Cluster', fontsize=25)plt.ylabel('Perception', fontsize=25)plt.grid(axis='y', linestyle='--', alpha=0.7)# Adjust layoutplt.tight_layout()# Show the plotplt.show()#%% Distribution clusters between W, MSE, N1, N2import pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltcol = 'Stage-at-probe' # Stage-at-probe # Stagegroups = ['2', '1','W'] # ['2', '1','MSE','W'] # ['2', '1','W']# Load table (clusters are included)csv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path, sep=None)cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# subs with behavioral issues are removedbehavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)scores = scores.dropna(subset=[col, 'Cluster'])# Merge MSE and N1scores[col] = scores[col].replace({'MSE': '1', 'N1': '1'})proportion_df = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0)proportion_df = proportion_df.div(proportion_df.sum(axis=1), axis=0)proportion_df = proportion_df.reindex(groups)# Create the bar plotfig, ax = plt.subplots(figsize=(16, 6))proportion_df.plot(kind='barh', stacked=True, ax=ax, color=[cluster_colors[col] for col in proportion_df.columns])# Customize plot aestheticsfontsize = 30ax.set_xlabel('% reports', fontsize=fontsize)ax.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])ax.set_xticklabels(['20', '40', '60', '80', '100'])ax.set_yticklabels(proportion_df.index, fontsize=fontsize-5)ax.tick_params(axis='x', labelsize=fontsize-5)ax.set_yticklabels(['N2', 'N1+MSE', 'W'], fontsize=fontsize-5)ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)legend_labels = ['C1 (fleeting)', 'C2 (alert)', 'C3 (bizarre)','C4 (voluntary)']plt.legend(legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=fontsize-5)ax.set_ylabel('')for bar in ax.patches:    bar.set_height(bar.get_height() * 1.5)total_counts = scores.groupby(col).size().reindex(groups, fill_value=0)for i, (group, total) in enumerate(total_counts.items()):    ax.text(1.02, i, f'{int(total)}', va='center', ha='left', fontsize=fontsize - 5)plt.show()plt.tight_layout()# Stat analysis### Chi Square to test whether overall, clusters change across stagesimport pandas as pdimport scipy.stats as stats# Define groups and clustersgroups = ['2', '1', 'W']  # Stagesclusters = [1, 2, 3, 4]  # Clusters# Create contingency table (Stage, Cluster) and run chisquarecontingency_table = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0)chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)print(f"P-value: {p_value:.4f}")from scipy.stats import chi2_contingencydef test_cluster_distribution_across_stages(contingency_table, cluster_id):    """    Tests whether a single cluster's distribution differs across stages.        Args:        contingency_table: pd.DataFrame with shape (Stage, Cluster)        cluster_id: int or str — the cluster to test (e.g., 2)        Returns:        chi2, p_value, dof, expected    """    # Extract just the counts for the cluster across stages    counts = contingency_table[cluster_id].values.reshape(-1, 1)  # column vector    # Make dummy 2-column table for chi2 test (observed vs expected under uniformity)    dummy_table = np.hstack([counts, contingency_table.sum(axis=1).values.reshape(-1, 1) - counts])    chi2, p_value, dof, expected = chi2_contingency(dummy_table)    return chi2, p_value, dof, expectedfor cluster in contingency_table.columns:    chi2, p, _, _ = test_cluster_distribution_across_stages(contingency_table, cluster)    print(f"Cluster {cluster}: p-value = {p:.4f}")        # Q1 --> Pariwise fischer testfrom scipy.stats import fisher_exactimport pandas as pdfrom itertools import productdef test_cluster_enrichment_in_stage(contingency_table):    results = []    stages = contingency_table.index    clusters = contingency_table.columns    for cluster in clusters:        for stage in stages:            A = contingency_table.loc[stage, cluster]            B = contingency_table.loc[stage].sum() - A            C = contingency_table[cluster].sum() - A            D = contingency_table.sum().sum() - (A + B + C)            table = [[A, B],                     [C, D]]            _, p_value = fisher_exact(table, alternative='greater')  # Test for overrepresentation            results.append({                "Cluster": cluster,                "Stage": stage,                "Count_in_stage": A,                "Count_elsewhere": C,                "Proportion_in_stage": A / (A + B) if A + B > 0 else 0,                "p_value": p_value,                "Table": f"[[{A},{B}], [{C},{D}]]"            })    return pd.DataFrame(results)results_df = test_cluster_enrichment_in_stage(contingency_table)results_df.sort_values("p_value", inplace=True)from statsmodels.stats.multitest import fdrcorrection_, corrected_pvals = fdrcorrection(results_df['p_value'])results_df['fdr_corrected_p'] = corrected_pvals# Q2 --> Chisquare goodness of fit import pandas as pdfrom scipy.stats import chisquare# Create contingency table (Stage x Cluster)groups = ['2', '1', 'W']  # Stagesclusters = [1, 2, 3, 4]  # Clusters# Create contingency table (Stage, Cluster) and run chisquarecontingency_table = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0)# Chi-squared test within each stageresults = []for stage in contingency_table.index:    observed = contingency_table.loc[stage].values    expected = [sum(observed) / len(observed)] * len(observed)  # uniform expectation    chi2_stat, p_value = chisquare(f_obs=observed, f_exp=expected)        results.append({        "Stage": stage,        "Chi2_stat": chi2_stat,        "p_value": p_value,        "Observed": observed.tolist(),        "Expected": expected    })results_df = pd.DataFrame(results)print(results_df)# FDRfrom statsmodels.stats.multitest import multipletestsraw_p = results_df["p_value"].values_, p_fdr, _, _ = multipletests(raw_p, method='fdr_bh')results_df["p_fdr"] = p_fdr# Posthoc: which cluster is over / underrepresented?import pandas as pdfrom scipy.stats import binomtestfrom statsmodels.stats.multitest import fdrcorrection# Recreate contingency_tablecontingency_table = pd.DataFrame({    1: [6, 26, 62],    2: [0, 10, 83],    3: [2, 16, 29],    4: [4, 19, 85]}, index=['2', '1', 'W'])# Post-hoc binomial test for Wake stagestage_name = 'W'observed = contingency_table.loc[stage_name].valuestotal = observed.sum()expected_prop = 1 / len(observed)  # 25% for 4 clusters# Perform one-sample binomial test for each cluster vs 25%posthoc_results = []for i, count in enumerate(observed):    p = binomtest(count, total, expected_prop, alternative='two-sided').pvalue    posthoc_results.append({        "Stage": stage_name,        "Cluster": contingency_table.columns[i],        "Count": count,        "Expected": total * expected_prop,        "Proportion": count / total,        "p_value": p    })# FDR correctionpvals = [r["p_value"] for r in posthoc_results]_, pvals_fdr = fdrcorrection(pvals)for i, fdr_p in enumerate(pvals_fdr):    posthoc_results[i]["FDR_corrected_p"] = fdr_pposthoc_df = pd.DataFrame(posthoc_results)#%% Proportion of cluster for each probe ######################################import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# Load table (clusters are included)csv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path)cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# subs with behavioral issues are removedbehavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)# Filter for nap1 probes (only 1a, 1b, 1c, and 1d)nap1_specific_probes = scores[scores['Probe'].isin(['1a', '1b', '1c', '1d'])]grouped_by_probe_count = nap1_specific_probes.groupby(['Probe', 'Cluster']).size().unstack(fill_value=0)grouped_by_probe_perc = grouped_by_probe_count.div(grouped_by_probe_count.sum(axis=1), axis=0) * 100# Plot stacked bar plot for countsax1 = grouped_by_probe_count.plot(kind='bar', stacked=True, figsize=(10, 6), color=[cluster_colors.get(i) for i in grouped_by_probe_count.columns])ax1.set_xlabel('Probe')ax1.set_ylabel('Count')ax1.set_title('Nap1: count cluster for each probe')ax1.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')plt.tight_layout()plt.show()# Plot stacked bar plot for percentagesax2 = grouped_by_probe_perc.plot(kind='bar', stacked=True, figsize=(10, 6), color=[cluster_colors.get(i) for i in grouped_by_probe_perc.columns])ax2.set_xlabel('Probe')ax2.set_ylabel('Percentage')ax2.set_title('Nap1: % cluster for each probe')ax2.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')plt.tight_layout()plt.show()# Nap2 probes (only 2a, 2b, 2c, and 2d)nap2_specific_probes = scores[scores['Probe'].isin(['2w', '2x', '2y', '2z'])]grouped_by_probe_count_nap2 = nap2_specific_probes.groupby(['Probe', 'Cluster']).size().unstack(fill_value=0)grouped_by_probe_perc_nap2 = grouped_by_probe_count_nap2.div(grouped_by_probe_count_nap2.sum(axis=1), axis=0) * 100# countsax1_nap2 = grouped_by_probe_count_nap2.plot(kind='bar', stacked=True, figsize=(10, 6), color=[cluster_colors.get(i) for i in grouped_by_probe_count_nap2.columns])ax1_nap2.set_xlabel('Probe')ax1_nap2.set_ylabel('Count')ax1_nap2.set_title('Nap2: count cluster for each probe')ax1_nap2.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')plt.tight_layout()plt.show()# percentagesax2_nap2 = grouped_by_probe_perc_nap2.plot(kind='bar', stacked=True, figsize=(10, 6), color=[cluster_colors.get(i) for i in grouped_by_probe_perc_nap2.columns])ax2_nap2.set_xlabel('Probe')ax2_nap2.set_ylabel('Percentage')ax2_nap2.set_title('Nap2: % cluster for each probe')ax2_nap2.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')plt.tight_layout()plt.show()#%% Time estimation in each cluster ###########################################import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# Load table (clusters are included)csv_path = '/Users/nico/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path)cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# subs with behavioral issues are removedbehavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)# Compute time estimation: estimated / objectivescores['Time estimation'] = pd.to_numeric(scores['Time estimation'], errors='coerce')scores['Duration'] = pd.to_numeric(scores['Duration'], errors='coerce')scores['time_estimation'] = scores['Time estimation']/scores['Duration']# Plot the distribution of time estimation across clustersplt.rcParams.update({'font.size': 18})cluster_colors = ['#ff5733', '#ffcc00', '#3366ff', '#cc33ff']  # Extend colors if necessarycluster_labels = [f'Cluster {i}' for i in sorted(scores['Cluster'].unique())]fig, ax = plt.subplots(figsize=(10, 10), facecolor='white')sns.violinplot(    x='Cluster',    y='time_estimation',    data=scores,    palette=cluster_colors[:len(cluster_labels)],    ax=ax)sns.stripplot(    x='Cluster',    y='time_estimation',    data=scores,    jitter=True,    dodge=True,    color='black',    size=4,    ax=ax)# Compute mean and median for each clustercluster_stats = scores.groupby('Cluster')['time_estimation'].agg(['mean', 'median'])for cluster_idx, (mean, median) in cluster_stats.iterrows():    ax.hlines(        y=mean,        xmin=cluster_idx -1 - 0.4,  # Adjust horizontal range for clarity        xmax=cluster_idx -1  + 0.4,        colors='blue',        linestyle='--',        linewidth=2,        label='Mean' if cluster_idx == 1 else ""  # Add legend only once    )# Customize plotax.set_ylabel('Time estimation (rel/obj)')ax.axhline(y=1, color='black', linestyle='--', label='Reference Line')ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)ax.set_xticklabels(cluster_labels, rotation=0)plt.legend(loc='upper left')plt.tight_layout()plt.show()#%% Recuperation level in each cluster ###########################################import pandas as pdimport seaborn as snsimport matplotlib.pyplot as plt# Load the datacsv_path = '/Users/nico/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path)# Remove behavioral issues and NaNs in 'Cluster'behavior = {"eyes-open", "trial-too-short", "intentional", "moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav).dropna(subset=['Cluster' ,'Recuperation'])# Ensure Cluster is treated as an integer (if applicable)scores['Cluster'] = scores['Cluster'].astype(int)# Dynamically map clusters to colors and labelsunique_clusters = sorted(scores['Cluster'].unique())cluster_colors = ['#ff5733', '#ffcc00', '#3366ff', '#cc33ff'][:len(unique_clusters)]cluster_labels = [f'Cluster {c}' for c in unique_clusters]# Plot setupplt.rcParams.update({'font.size': 18})fig, ax = plt.subplots(figsize=(10, 10), facecolor='white')# Violin plot with stripplot overlaysns.violinplot(    x='Cluster',    y='Recuperation',    data=scores,    palette=cluster_colors,    order=unique_clusters,  # Ensure consistent ordering    ax=ax)sns.stripplot(    x='Cluster',    y='Recuperation',    data=scores,    jitter=True,    dodge=True,    color='black',    size=8,    order=unique_clusters,  # Match the same ordering    ax=ax)# Compute and plot mean for each clustermean_recuperation_across_clusters = scores.groupby('Cluster')['Recuperation'].mean()for cluster_idx, mean in mean_recuperation_across_clusters.items():    ax.hlines(        y=mean,        xmin=cluster_idx - 1 - 0.4,  # Adjust horizontal range to align with cluster        xmax=cluster_idx - 1 + 0.4,        colors='blue',        linestyle='--',        linewidth=2,        label='Mean' if cluster_idx == unique_clusters[0] else ""  # Add legend only once    )# Customize plotax.set_ylabel('Recuperation level')ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)ax.set_xticks(range(len(unique_clusters)))ax.set_xticklabels(cluster_labels, rotation=0)  # Ensure labels are correctplt.legend(loc='upper left')plt.tight_layout()plt.show()import statsmodels.api as smfrom statsmodels.formula.api import olsfrom statsmodels.stats.multicomp import pairwise_tukeyhsd# Perform ANOVAanova_model = ols('Recuperation ~ C(Cluster)', data=scores).fit()anova_table = sm.stats.anova_lm(anova_model, typ=2)print(anova_table)# Tukey HSD post-hoc testtukey = pairwise_tukeyhsd(endog=scores['Recuperation'], groups=scores['Cluster'], alpha=0.05)print(tukey.summary())#%% Measuring subj dimension redundancy by computing correlation matriximport matplotlib as mplcmap = mpl.colormaps['coolwarm']correlation_matrix = scores[["Bizarreness", "Fluidity", "Spontaneity","Wake level"]].corr()sns.set(style="white")sns.set(font_scale=2.3)  # Increase the font scaleplt.figure(figsize=(10, 8))# Plot the correlation matrix as a heatmap with customized colorbarheatmap = sns.heatmap(correlation_matrix, cmap=cmap, annot=True, fmt=".2f", square=True, vmin=-1, vmax=1,                      cbar_kws={'ticks': [-1, -0.5, 0, 0.5, 1], 'label': 'Correlation'})# Adjust the colorbar to make it shorter and remove tickscbar = heatmap.collections[0].colorbarcbar.set_ticks([-1, -0.5, 0, 0.5, 1])cbar.ax.set_yticklabels(['-1', '-0.5', '0', '0.5', '1'])  # Set the labelscbar.ax.tick_params(length=0)  # Hide the ticks# Shift x tick labels slightly to the leftfor label in heatmap.get_xticklabels():    label.set_horizontalalignment('right')    label.set_x(label.get_position()[0] - 0.07)  # Adjust the value as needed to shift leftplt.xticks(rotation=45)plt.yticks(rotation=0)plt.tight_layout()plt.show()# Compute mean corrupper_tri_values = correlation_matrix.values[np.triu_indices_from(correlation_matrix, k=1)]mean_abs_correlation = np.mean(np.abs(upper_tri_values))std_abs_correlation = np.std(np.abs(upper_tri_values))################################################################################## CLUSTERS EEG  #############################################################################################################################################%% PSD across sleep stages and clustersimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltpath_table = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'path_eeg = '/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv'# Load tablegigatable = pd.read_csv(path_table) eeg = pd.read_csv(path_eeg)# Parameterschans = "midline" # "allchans", "midline"idx_fooof = "" # "" "_ff"midchans = ["Fz", "CPz", 'Oz']stages = ['W', '1', '2', 'MSE']stage_colors = {'W': 'red','MSE': 'purple','1': 'lightblue','2': 'darkblue'}clusters = [1, 2, 3, 4]cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# Convert psd_datadef psd_to_array(psd_str):    psd_str = psd_str.replace('[', '').replace(']', '').replace('\n', '')    return np.array([float(val) for val in psd_str.split()])eeg[f'psd_data{idx_fooof}'] = eeg[f'psd_data{idx_fooof}'].apply(psd_to_array)# PLOT PSD ACROSS STAGESif chans == "allchans":        # Calculate the average psd across channels for each subject    eeg_avg_psd = eeg.groupby('subject')[f'psd_data{idx_fooof}'].apply(lambda x: np.mean(np.vstack(x), axis=0)).reset_index()    eeg_avg_psd.rename(columns={f'psd_data{idx_fooof}': 'avg_psd_data'}, inplace=True)        # Add the averaged psd data to eeg    eeg = eeg.merge(eeg_avg_psd[['subject', 'avg_psd_data']], on='subject', how='left')    eeg = eeg[['subject', 'avg_psd_data']]    eeg = eeg.drop_duplicates(subset='subject', keep='first')        # Merge eeg and gigatable    gigatable['subject_key'] = gigatable['Subject'] + '_' + gigatable['Probe'] + '_' + gigatable['Group']    merged_df = pd.merge(gigatable, eeg[['subject','avg_psd_data']], left_on='subject_key', right_on='subject', how='left')    merged_df = merged_df.drop(columns=['subject'])        grouped_data = {}    for stage in stages:        group = merged_df[merged_df['Stage-at-probe'] == stage]        # Calculate mean across subjects        avg_psd = np.mean(np.vstack(group['avg_psd_data'].dropna().values), axis=0)        if idx_fooof == "": # Convert to dB            avg_psd = 10 * np.log10(avg_psd + 1e-12)  # Add small value to avoid log of zero        grouped_data[stage] = avg_psd        plt.figure(figsize=(12, 6))    freqs = np.linspace(0, 40, len(avg_psd))  # Assuming 0–40 Hz range for simplicity; adjust as needed    for stage in ['W', 'MSE', '1', '2']:  # Ensure the stages are plotted in the desired order        psd_data = grouped_data[stage]        plt.plot(freqs, psd_data, label=f'{stage} (n={len(merged_df[merged_df["Stage-at-probe"] == stage])})',                 color=stage_colors[stage],linewidth=2.5)    ax = plt.gca()    ax.spines['right'].set_visible(False)    ax.spines['top'].set_visible(False)    plt.title(f'{chans}{idx_fooof}')    plt.xlabel('Frequency (Hz)')    plt.ylabel('PSD')    plt.legend()    plt.show()    elif chans == "midline":        fig, axes = plt.subplots(len(midchans), 1, figsize=(12, 6 * len(midchans)), sharex=True)        for idx, midchan in enumerate(midchans):                ax = axes[idx]                # Get the psd_data for the current midline channel        eeg_midchan_psd = eeg[eeg['channel'] == midchan].groupby('subject')[f'psd_data{idx_fooof}'].first().reset_index()        eeg_midchan_psd.rename(columns={f'psd_data{idx_fooof}': 'midchan_psd_data'}, inplace=True)                # Add the averaged psd data to eeg        eeg = eeg.merge(eeg_midchan_psd[['subject', 'midchan_psd_data']], on='subject', how='left')        eeg_select = eeg_midchan_psd[['subject', 'midchan_psd_data']]        eeg_select = eeg_select.drop_duplicates(subset='subject', keep='first')                # Merge eeg and gigatable        gigatable['subject_key'] = gigatable['Subject'] + '_' + gigatable['Probe'] + '_' + gigatable['Group']        merged_df = pd.merge(gigatable, eeg_select[['subject','midchan_psd_data']], left_on='subject_key', right_on='subject', how='left')        merged_df = merged_df.drop(columns=['subject'])                # PLOT PSD ACROSS STAGES        grouped_data = {}        for stage in stages:            group = merged_df[merged_df['Stage-at-probe'] == stage]            # Calculate mean across subjects            avg_psd = np.mean(np.vstack(group['midchan_psd_data'].dropna().values), axis=0)            if idx_fooof == "": # Convert to dB                avg_psd = 10 * np.log10(avg_psd + 1e-12)  # Add small value to avoid log of zero            grouped_data[stage] = avg_psd                freqs = np.linspace(0, 40, len(avg_psd))  # Assuming 0–40 Hz range for simplicity; adjust as needed        for stage in stages:            psd_data = grouped_data[stage]            ax.plot(freqs, psd_data, label=f'{stage} (n={len(merged_df[merged_df["Stage-at-probe"] == stage])})',                    color=stage_colors[stage], linewidth=2.5)                # Customize the subplot        ax.spines['right'].set_visible(False)        ax.spines['top'].set_visible(False)        ax.set_title(f'{midchan}{idx_fooof}')        ax.set_xlabel('Frequency (Hz)')        ax.set_ylabel('PSD')        ax.tick_params(labelbottom=True)        ax.legend()            plt.tight_layout()plt.show()              # PLOT PSD ACROSS CLUSTERSeeg = pd.read_csv(path_eeg)eeg[f'psd_data{idx_fooof}'] = eeg[f'psd_data{idx_fooof}'].apply(psd_to_array)if chans == "allchans":        grouped_data = {}        for cluster in clusters:        group = merged_df[merged_df['Cluster'] == cluster]        # Calculate mean across subjects        avg_psd = np.mean(np.vstack(group['avg_psd_data'].dropna().values), axis=0)        if idx_fooof == "": # Convert to dB            avg_psd = 10 * np.log10(avg_psd + 1e-12)        grouped_data[cluster] = avg_psd        # Plot the PSD across clusters    plt.figure(figsize=(12, 6))    freqs = np.linspace(0, 40, len(avg_psd))  # Assuming 0–40 Hz range for simplicity; adjust as needed    for cluster, psd_data in grouped_data.items():        plt.plot(freqs, psd_data, label=f'Cluster {cluster} (n={len(merged_df[merged_df["Cluster"] == cluster])})',                 color=cluster_colors[cluster], linewidth=2.5)  # Thicker line and cluster colors    plt.title(f'{chans}{idx_fooof}')    plt.xlabel('Frequency (Hz)')    plt.ylabel('PSD (dB)')        # Remove the right and top frame lines    ax = plt.gca()    ax.spines['right'].set_visible(False)    ax.spines['top'].set_visible(False)        # Add legend and show the plot    plt.legend()    plt.show()        elif chans == "midline":        eeg_midchan_psd = []    fig, axes = plt.subplots(len(midchans), 1, figsize=(12, 6 * len(midchans)), sharex=True)        # Loop through midchans and plot on each subplot    for idx, midchan in enumerate(midchans):                ax = axes[idx]                  # Get the psd_data for the current midline channel        eeg_midchan_psd = eeg[eeg['channel'] == midchan].groupby('subject')[f'psd_data{idx_fooof}'].first().reset_index()        eeg_midchan_psd.rename(columns={f'psd_data{idx_fooof}': 'midchan_psd_data'}, inplace=True)                # Add the averaged psd data to eeg        eeg = eeg.merge(eeg_midchan_psd[['subject', 'midchan_psd_data']], on='subject', how='left')        eeg_select = eeg_midchan_psd[['subject', 'midchan_psd_data']]        eeg_select = eeg_select.drop_duplicates(subset='subject', keep='first')                # Merge eeg and gigatable        gigatable['subject_key'] = gigatable['Subject'] + '_' + gigatable['Probe'] + '_' + gigatable['Group']        merged_df = pd.merge(gigatable, eeg_select[['subject','midchan_psd_data']], left_on='subject_key', right_on='subject', how='left')        merged_df = merged_df.drop(columns=['subject'])                grouped_data = {}                for cluster in clusters:            group = merged_df[merged_df['Cluster'] == cluster]            # Calculate mean across subjects            avg_psd = np.mean(np.vstack(group['midchan_psd_data'].dropna().values), axis=0)            if idx_fooof == "": # Convert to dB                avg_psd = 10 * np.log10(avg_psd + 1e-12)            grouped_data[cluster] = avg_psd                freqs = np.linspace(0, 40, len(avg_psd))  # Adjust frequency range as needed        for cluster, psd_data in grouped_data.items():            ax.plot(freqs, psd_data, label=f'Cluster {cluster} (n={len(merged_df[merged_df["Cluster"] == cluster])})',                    color=cluster_colors[cluster], linewidth=2.5)  # Thicker line and cluster colors        ax.spines['right'].set_visible(False)        ax.spines['top'].set_visible(False)        ax.set_title(f'{midchan}{idx_fooof}')        ax.set_ylabel('PSD (dB)')        ax.tick_params(labelbottom=True)        ax.legend()    # Add a single shared x-axis label    fig.text(0.5, 0.04, 'Frequency (Hz)', ha='center', fontsize=14)        # Adjust layout to prevent overlapping    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space for the shared x-axis label    plt.show()        #%% Diff in relative power across clustersimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltimport seaborn as sns# 1) Get cluster distribcol = 'Stage-at-probe' # Stage-at-probe # Stagegroups = ['2','1','W'] # ['2', '1','MSE','W'] # ['2', '1','W']csv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path)behavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)scores = scores.dropna(subset=[col, 'Cluster'])scores[col] = scores[col].replace({'MSE': '1', 'N1': '1'})proportion_df = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0)proportion_df = proportion_df.div(proportion_df.sum(axis=1), axis=0)proportion_df = proportion_df.reindex(groups)# Reduce scores to only relevant colsscores = scores[["Subject","Probe","Group","Stage-at-probe","Cluster"]]# 2) Import and merge table + eeg# path_eeg = "/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv"path_eeg = "/Users/nicolas.decat/Downloads/eeg_features_10-0.csv"eeg = pd.read_csv(path_eeg, index_col=0)eeg[['Subject', 'Probe', 'Group']] = eeg['subject'].str.split('_', expand=True)eeg = eeg.merge(table,                 left_on=['Subject', 'Probe', 'Group'],                 right_on=['Subject', 'Probe', 'Group'],                 how='left')# Average across channelseeg = eeg[["subject","rel_delta_ff","rel_alpha_ff"]]avg_eeg = eeg.groupby("subject", as_index=False).agg({    "rel_delta_ff": "mean",    "rel_alpha_ff": "mean",})# Merge eeg and infoscores['merged_id'] = scores['Subject'].astype(str) + '_' + scores['Probe'].astype(str) + '_' + scores['Group'].astype(str)merged = pd.merge(scores, avg_eeg[['subject', 'rel_delta_ff', 'rel_alpha_ff']],                   left_on='merged_id', right_on='subject', how='left')merged.drop(columns=['merged_id', 'subject'], inplace=True).dropna(subset=['rel_delta_ff', 'rel_alpha_ff'], inplace=True)# FOR EACH STAGE: PLOT METRICS ACROSS CLUSTERSimport seaborn as snsimport matplotlib.pyplot as plt# Custom stylesns.set(style="ticks")  # no gridcluster_colors = {1: '#ff5733', 2: '#ffcc00', 3: '#3366ff', 4: '#cc33ff'}palette = [cluster_colors[c] for c in sorted(cluster_colors)]# Custom stage labelsstage_labels = {"W": "Wake", "1": "N1", "2": "N2"}stages = merged['Stage-at-probe'].unique()n_stages = len(stages)# Set up figurefig, axes = plt.subplots(nrows=2, ncols=n_stages, figsize=(5 * n_stages, 10), sharey='row')# delta rowfor i, stage in enumerate(stages):    ax = axes[0, i]    stage_data = merged[merged['Stage-at-probe'] == stage]    sns.boxplot(data=stage_data, x='Cluster', y='rel_delta_ff', palette=palette, ax=ax, showfliers=False)    sns.stripplot(data=stage_data, x='Cluster', y='rel_delta_ff', color='black', alpha=0.5, jitter=True, ax=ax)    ax.set_title(stage_labels.get(str(stage), stage))    ax.set_xlabel("")    ax.set_ylabel("")    ax.spines['top'].set_visible(False)    ax.spines['right'].set_visible(False)# alpha rowfor i, stage in enumerate(stages):    ax = axes[1, i]    stage_data = merged[merged['Stage-at-probe'] == stage]    sns.boxplot(data=stage_data, x='Cluster', y='rel_alpha_ff', palette=palette, ax=ax, showfliers=False)    sns.stripplot(data=stage_data, x='Cluster', y='rel_alpha_ff', color='black', alpha=0.5, jitter=True, ax=ax)    ax.set_title(stage_labels.get(str(stage), stage))    ax.set_xlabel("")    ax.set_ylabel("")    ax.spines['top'].set_visible(False)    ax.spines['right'].set_visible(False)# Set y-axis labelsaxes[0, 0].set_ylabel("delta")axes[1, 0].set_ylabel("alpha")plt.tight_layout()plt.show()# FOR EACH STAGE: PLOT METRICS ACROSS CLUSTERSimport seaborn as snsimport matplotlib.pyplot as plt# Custom stylesns.set(style="ticks")# Cluster colorscluster_colors = {1: '#ff5733', 2: '#ffcc00', 3: '#3366ff', 4: '#cc33ff'}palette = [cluster_colors[k] for k in sorted(cluster_colors)]# Stage labelsstage_labels = {'W': 'Wake', '1': 'N1', '2': 'N2'}stage_order = ['W', '1', '2']cluster_order = [1, 2, 3, 4]# Helper function to create boxplot griddef plot_contrast_grid(band_col, band_label):    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(6, 12), sharey=True)    for i, cluster in enumerate(cluster_order):        for j, stage in enumerate(stage_order):            ax = axes[i, j]            data = merged[(merged['Cluster'] == cluster) & (merged['Stage-at-probe'] == stage)]            sns.boxplot(data=data, y=band_col, color=cluster_colors[cluster], ax=ax, showfliers=False)            sns.stripplot(data=data, y=band_col, color='black', alpha=0.5, jitter=True, ax=ax)            # Set the title only for the top row            if i == 0:                ax.set_title(f"Stage {stage_labels[stage]}")            else:                ax.set_title("")            ax.set_xlabel("")            ax.set_ylabel("")  # Don't set y-label here, we'll set it later            # Clean up style            ax.spines['top'].set_visible(False)            ax.spines['right'].set_visible(False)            ax.set_xticks([])  # Remove x-axis ticks            ax.set_xlabel("")        # Set the cluster label only on the leftmost column (row-wise)        axes[i, 0].set_ylabel(f"Cluster {cluster}", fontsize=12, fontweight='bold')        for j in range(1, 3):  # Remove y-labels on non-leftmost subplots            axes[i, j].set_ylabel("")    # Set the overall title for the figure    fig.suptitle(f"{band_label}", fontsize=16, fontweight='bold')    plt.tight_layout()    plt.show()# Plot deltaplot_contrast_grid("rel_delta_ff", "delta")# Plot alphaplot_contrast_grid("rel_alpha_ff", "alpha")# stat testsimport scipy.stats as statsfrom statsmodels.stats.multitest import multipletests# Clean up stage names for claritymerged['Stage-at-probe'] = merged['Stage-at-probe'].replace({'W': 'Wake', '1': 'N1', '2': 'N2'})# We'll test both delta and alpha across sleep stages, within each clusterresults = []for band in ['rel_delta_ff', 'rel_alpha_ff']:    for cluster in sorted(merged['Cluster'].unique()):        subset = merged[merged['Cluster'] == cluster]        stages = subset['Stage-at-probe'].unique()        # Group data by sleep stage        data_by_stage = [subset[subset['Stage-at-probe'] == stage][band].dropna() for stage in stages]                # Use Kruskal-Wallis test (non-parametric, safe with unequal sizes)        if len(data_by_stage) > 1:            H, p = stats.kruskal(*data_by_stage)        else:            H, p = None, None        # Save results        results.append({            'band': band,            'cluster': cluster,            'H_statistic': H,            'p_value': p,            'stages_compared': ', '.join(stages)        })# Convert results to DataFrameresults_df = pd.DataFrame(results)# Apply FDR correction for multiple comparisons_, corrected_p, _, _ = multipletests(results_df['p_value'], method='fdr_bh')results_df['p_corrected'] = corrected_presults_df.sort_values(['band', 'cluster'], inplace=True)results_df.reset_index(drop=True, inplace=True)# Post hoc tests to see which pairs are significantimport pandas as pdfrom scipy.stats import mannwhitneyufrom statsmodels.stats.multitest import multipletestsimport itertools# Ensure necessary columns existif 'Stage-at-probe' in merged.columns and 'Cluster' in merged.columns:    # Store all pairwise comparisons    posthoc_results = []    for band in ['rel_delta_ff', 'rel_alpha_ff']:        if band not in merged.columns:            continue  # skip missing bands        for cluster in sorted(merged['Cluster'].dropna().unique()):            subset = merged[merged['Cluster'] == cluster]            stages = subset['Stage-at-probe'].dropna().unique()            if len(stages) > 1:                for s1, s2 in itertools.combinations(stages, 2):                    data1 = subset[subset['Stage-at-probe'] == s1][band].dropna()                    data2 = subset[subset['Stage-at-probe'] == s2][band].dropna()                    if len(data1) > 0 and len(data2) > 0:                        stat, p = mannwhitneyu(data1, data2, alternative='two-sided')                        posthoc_results.append({                            'band': band,                            'cluster': cluster,                            'stage_1': s1,                            'stage_2': s2,                            'p_value': p                        })    # Convert to DataFrame    posthoc_df = pd.DataFrame(posthoc_results)    # Apply FDR correction within each band/cluster block    posthoc_df['p_corrected'] = None    posthoc_df['significant'] = False    for (band, cluster), group_df in posthoc_df.groupby(['band', 'cluster']):        corrected = multipletests(group_df['p_value'], method='fdr_bh')        posthoc_df.loc[group_df.index, 'p_corrected'] = corrected[1]        posthoc_df.loc[group_df.index, 'significant'] = corrected[0]else:    posthoc_df = pd.DataFrame(columns=['band', 'cluster', 'stage_1', 'stage_2', 'p_value', 'p_corrected', 'significant'])#%% Content vs. non content # Distribution of clusters across all 5 clustersimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltcol = 'Stage-at-probe' # Stage-at-probe # Stagegroups = ['2', '1','W'] # ['2', '1','MSE','W'] # ['2', '1','W']dimensions = ['Bizarreness', 'Fluidity', 'Spontaneity', 'Wake level']# Load table (clusters are included)csv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path)cluster_colors = {0: '#333333',1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# subs with behavioral issues are removedbehavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)scores_nc = scoresscores = scores.dropna(subset=[col, 'Cluster'])#################################### ADD THE NON CONTENT SCORESscores_nc = scores_nc.dropna(subset=dimensions)# keep subs who had no content (0 in all dimensions)subs_no_content = scores_nc.index[scores_nc[dimensions[:-1]].sum(axis=1) == 0].tolist()scores_nc = scores_nc.loc[subs_no_content]scores_nc['Cluster'] = 0scores_nc = scores_nc.dropna(subset=[col, 'Stage-at-probe'])# Merge scores and scores_ncscores = pd.concat([scores, scores_nc], ignore_index=True)#################################### Merge MSE and N1scores[col] = scores[col].replace({'MSE': '1', 'N1': '1'})# Compute proportions# Compute proportionsproportion_df = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0)proportion_df = proportion_df.div(proportion_df.sum(axis=1), axis=0)proportion_df = proportion_df.reindex(groups)# Create the bar plotfig, ax = plt.subplots(figsize=(14, 8))proportion_df.plot(    kind='barh',    stacked=True,    ax=ax,    color=[cluster_colors[c] for c in proportion_df.columns])# Plot settingsfontsize = 30ax.set_xlim(0, 1.0)ax.set_xlabel('Percentage', fontsize=fontsize)ax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])ax.set_xticklabels(['0', '20', '40', '60', '80', '100'])ax.tick_params(axis='x', labelsize=fontsize - 5)# Remove top, right, left spinesfor spine in ['right', 'top', 'left']:    ax.spines[spine].set_visible(False)# Legend (outside, no frame)legend_labels = ['C0 (no content)', 'C1 (fleeting)', 'C2 (alert)', 'C3 (bizarre)', 'C4 (voluntary)']ax.legend(    legend_labels,    bbox_to_anchor=(1.15, 1),  # shifted from 1.05 to 1.15    loc='upper left',    fontsize=fontsize - 5,    frameon=False)# Enlarge bars and adjust original heightsbar_height_factor = 1.5original_heights = []for bar in ax.patches:    original_heights.append(bar.get_height())    bar.set_height(bar.get_height() * bar_height_factor)# Adjust y-tick label positions to center them vertically on expanded barsadjusted_y_ticks = [i + (original_heights[i] * (bar_height_factor - 1) / 2) for i in range(len(groups))]ax.set_yticks(adjusted_y_ticks)ax.set_yticklabels(['N2', 'N1', 'W'], fontsize=fontsize - 5)# Total counts at end of each bartotal_counts = scores.groupby(col).size().reindex(groups, fill_value=0)for i, (group, total) in enumerate(total_counts.items()):    ax.text(1.02, adjusted_y_ticks[i], f'{int(total)}', va='center', ha='left', fontsize=fontsize - 5)# Cluster counts inside segmentsraw_counts = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0).reindex(groups)for i, group in enumerate(groups):    adjusted_y = adjusted_y_ticks[i]    cum_width = 0    for c in proportion_df.columns:        width = proportion_df.loc[group, c]        count = raw_counts.loc[group, c]        if width > 0:            text_color = 'white' if c == 0 else 'black'            ax.text(                cum_width + width / 2,                adjusted_y,                str(int(count)),                ha='center',                va='center',                fontsize=fontsize - 7,                color=text_color            )            cum_width += widthplt.tight_layout()plt.show()# plt.savefig("/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/figures/supp_fig/co-content_distribution.png", dpi=300, bbox_inches='tight')#%% PSD with no-content clusterimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltpath_table = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'path_eeg = '/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv'table = pd.read_csv(path_table)# Load tablegigatable = pd.read_csv(path_table) #################################### ADD THE NON CONTENT SCORES# add value 0 to Cluster that are no contentdimensions = ['Bizarreness', 'Fluidity', 'Spontaneity','Wake level']behavior = {"eyes-open", "trial-too-short","intentional","moving"}subs_no_content_table = table[table[dimensions[:-1]].sum(axis=1) == 0]sub_behav = subs_no_content_table[subs_no_content_table['Behavior'].isin(behavior)].index.tolist()subs_no_content_table_excluded = subs_no_content_table.drop(sub_behav)subs_no_content_table_excluded = subs_no_content_table_excluded.dropna(subset=['Trial start'])subs_no_content = subs_no_content_table_excluded.index[subs_no_content_table_excluded[dimensions[:-1]].sum(axis=1) == 0].tolist()gigatable.loc[subs_no_content, 'Cluster'] = 0###################################eeg = pd.read_csv(path_eeg)# Parameterschans = "roi" # "allchans", "midline"idx_fooof = "" # "" "_ff"midchans = ["Fz", "CPz", 'Oz']stages = ['W', '1', '2', 'MSE']stage_colors = {'W': 'red','MSE': 'purple','1': 'lightblue','2': 'darkblue'}clusters = [0, 1, 2, 3, 4]cluster_colors = {0: '#333333',1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# Convert psd_datadef psd_to_array(psd_str):    psd_str = psd_str.replace('[', '').replace(']', '').replace('\n', '')    return np.array([float(val) for val in psd_str.split()])eeg[f'psd_data{idx_fooof}'] = eeg[f'psd_data{idx_fooof}'].apply(psd_to_array)roi_names = ['Frontal', 'Central', 'Occipital']frontal_channels = ['Fp1', 'Fp2', 'Fpz', 'AF3', 'AF4', 'AFz', 'F3', 'F4', 'Fz']central_channels = ['FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C1', 'C2', 'C3', 'C4', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4']occipital_channels = ['P3', 'P1', 'Pz', 'P2', 'P4', 'PO3', 'PO1', 'POz', 'PO2', 'PO4', 'Oz', 'O1', 'O2']roi_channels = [frontal_channels, central_channels, occipital_channels]    fig, axes = plt.subplots(1,len(roi_names), figsize=(12, 6 * len(roi_names)), sharex=True) # Loop through ROIs and plot on each subplotfor idx, (roi_name, channels) in enumerate(zip(roi_names, roi_channels)):    ax = axes[idx]        # Get the psd_data for all channels in the current ROI and average them    roi_psds = []    for subject in eeg['subject'].unique():        # Get PSD data for all channels in ROI for this subject        subject_channels_psd = eeg[(eeg['subject'] == subject) &                                  (eeg['channel'].isin(channels))][f'psd_data{idx_fooof}'].values        # Average across channels        avg_psd = np.mean(subject_channels_psd, axis=0)        roi_psds.append(avg_psd)        # Create DataFrame with averaged ROI data    eeg_roi_psd = pd.DataFrame({        'subject': eeg['subject'].unique(),        'roi_psd_data': roi_psds    })        # Merge with gigatable    gigatable['subject_key'] = gigatable['Subject'] + '_' + gigatable['Probe'] + '_' + gigatable['Group']    merged_df = pd.merge(gigatable, eeg_roi_psd[['subject', 'roi_psd_data']],                         left_on='subject_key', right_on='subject', how='left')    merged_df = merged_df.drop(columns=['subject'])        grouped_data = {}    for cluster in clusters:        group = merged_df[merged_df['Cluster'] == cluster]        avg_psd = np.mean(np.vstack(group['roi_psd_data'].dropna().values), axis=0)                if idx_fooof == "":            # Convert to dB            avg_psd = 10 * np.log10(avg_psd + 1e-12)                grouped_data[cluster] = avg_psd        freqs = np.linspace(0, 40, len(avg_psd))        # Remove y-axis labels & ticks for 2nd and 3rd subplot    if idx > 0:        ax.set_ylabel('')        ax.set_yticklabels([])        ax.tick_params(axis='y', left=False)        for cluster, psd_data in grouped_data.items():        ax.plot(freqs, psd_data,                 label=f'Cluster {cluster} (n={len(merged_df[merged_df["Cluster"] == cluster])})',                color=cluster_colors[cluster],                linewidth=2.5,                zorder=10 if cluster == 0 else 1)  # <-- ADD THIS        ax.spines['right'].set_visible(False)    ax.spines['top'].set_visible(False)    ax.set_title(f'{roi_name}')    if idx == 0:        ax.set_ylabel('PSD (dB)')    ax.tick_params(labelbottom=True)    # Add shared x-axis labelfig.text(0.5, 0.04, 'Frequency (Hz)', ha='center')# Adjust layoutplt.subplots_adjust(wspace=0.1)plt.tight_layout(rect=[0, 0.05, 1, 1])plt.show()        # plt.savefig("/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/figures/supp_fig/co-content_psd.png", dpi=300, bbox_inches='tight')#%% Alpha and theta power across stages and clustersimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltimport seaborn as sns# 1) Get cluster distribcol = 'Stage-at-probe' # Stage-at-probe # Stagegroups = ['2','1','W'] # ['2', '1','MSE','W'] # ['2', '1','W']csv_path = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'table = pd.read_csv(csv_path, sep=None)behavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()scores = table.drop(sub_behav)scores = scores.dropna(subset=[col, 'Cluster'])scores[col] = scores[col].replace({'MSE': '1', 'N1': '1'})proportion_df = scores.groupby([col, 'Cluster']).size().unstack(fill_value=0)proportion_df = proportion_df.div(proportion_df.sum(axis=1), axis=0)proportion_df = proportion_df.reindex(groups)# Reduce scores to only relevant colsscores = scores[["Subject","Probe","Group","Stage-at-probe","Cluster"]]# 2) Import and merge table + eegpath_eeg = "/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv"eeg = pd.read_csv(path_eeg, index_col=0)eeg[['Subject', 'Probe', 'Group']] = eeg['subject'].str.split('_', expand=True)eeg = eeg.merge(table,                 left_on=['Subject', 'Probe', 'Group'],                 right_on=['Subject', 'Probe', 'Group'],                 how='left')# Average across channelseeg = eeg[["subject","rel_delta_ff","rel_alpha_ff"]]avg_eeg = eeg.groupby("subject", as_index=False).agg({    "rel_delta_ff": "mean",    "rel_alpha_ff": "mean",})# Merge eeg and infoscores['merged_id'] = scores['Subject'].astype(str) + '_' + scores['Probe'].astype(str) + '_' + scores['Group'].astype(str)merged = pd.merge(scores, avg_eeg[['subject', 'rel_delta_ff', 'rel_alpha_ff']],                   left_on='merged_id', right_on='subject', how='left')merged.drop(columns=['merged_id', 'subject'], inplace=True)merged.dropna(subset=['rel_delta_ff', 'rel_alpha_ff'], inplace=True)# FOR EACH STAGE: PLOT METRICS ACROSS CLUSTERSimport seaborn as snsimport matplotlib.pyplot as plt# Custom stylesns.set(style="ticks")  # no gridcluster_colors = {1: '#ff5733', 2: '#ffcc00', 3: '#3366ff', 4: '#cc33ff'}palette = [cluster_colors[c] for c in sorted(cluster_colors)]# Custom stage labelsstage_labels = {"W": "Wake", "1": "N1", "2": "N2"}stages = merged['Stage-at-probe'].unique()n_stages = len(stages)# Set up figurefig, axes = plt.subplots(nrows=2, ncols=n_stages, figsize=(5 * n_stages, 10), sharey='row')# delta rowfor i, stage in enumerate(stages):    ax = axes[0, i]    stage_data = merged[merged['Stage-at-probe'] == stage]    sns.boxplot(data=stage_data, x='Cluster', y='rel_delta_ff', palette=palette, ax=ax, showfliers=False)    sns.stripplot(data=stage_data, x='Cluster', y='rel_delta_ff', color='black', alpha=0.5, jitter=True, ax=ax)    ax.set_title(stage_labels.get(str(stage), stage))    ax.set_xlabel("")    ax.set_ylabel("")    ax.spines['top'].set_visible(False)    ax.spines['right'].set_visible(False)# alpha rowfor i, stage in enumerate(stages):    ax = axes[1, i]    stage_data = merged[merged['Stage-at-probe'] == stage]    sns.boxplot(data=stage_data, x='Cluster', y='rel_alpha_ff', palette=palette, ax=ax, showfliers=False)    sns.stripplot(data=stage_data, x='Cluster', y='rel_alpha_ff', color='black', alpha=0.5, jitter=True, ax=ax)    ax.set_title(stage_labels.get(str(stage), stage))    ax.set_xlabel("")    ax.set_ylabel("")    ax.spines['top'].set_visible(False)    ax.spines['right'].set_visible(False)# Set y-axis labelsaxes[0, 0].set_ylabel("delta")axes[1, 0].set_ylabel("alpha")plt.tight_layout()plt.show()# FOR EACH STAGE: PLOT METRICS ACROSS CLUSTERSimport seaborn as snsimport matplotlib.pyplot as pltimport seaborn as snsimport matplotlib.pyplot as pltsns.set(style="ticks")# Colors and orders (unchanged)cluster_colors = {1: '#ff5733', 2: '#ffcc00', 3: '#3366ff', 4: '#cc33ff'}stage_labels = {'W': 'Wake', '1': 'N1', '2': 'N2'}stage_order = ['W', '1', '2']cluster_order = [1, 2, 3, 4]all_color = '#cccccc'  # light grey for the "All clusters" columndef plot_contrast_grid_combined(band_cols, band_labels):    # now 5 columns (4 clusters + All)    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(22, 8), sharey='row')    extended_cols = cluster_order + ['All']    for row_idx, (band_col, band_label) in enumerate(zip(band_cols, band_labels)):        for col_idx, col_key in enumerate(extended_cols):            ax = axes[row_idx, col_idx]            if col_key == 'All':                cluster_data = merged.copy()                pal = [all_color] * len(stage_order)                title_txt = 'All clusters'            else:                cluster_data = merged[merged['Cluster'] == col_key]                pal = [cluster_colors[col_key]] * len(stage_order)                title_txt = f'Cluster {col_key}'            # Boxplot            sns.boxplot(                data=cluster_data,                x='Stage-at-probe',                y=band_col,                order=stage_order,                palette=pal,                ax=ax,                showfliers=False            )            # Jittered points            sns.stripplot(                data=cluster_data,                x='Stage-at-probe',                y=band_col,                order=stage_order,                color='black',                alpha=0.5,                jitter=True,                ax=ax            )            # Titles on top row            if row_idx == 0:                ax.set_title(title_txt, fontsize=16)            # Y-axis labeling only on first column of each row            if col_idx == 0:                ax.set_ylabel(band_label, fontsize=16)                ax.tick_params(axis='y', labelsize=16)            else:                ax.set_ylabel('')                ax.tick_params(axis='y', left=False, labelleft=False)            ax.set_xlabel('')            ax.tick_params(axis='x', labelsize=16)            ax.set_xticklabels([stage_labels[s] for s in stage_order])            # Clean style            ax.spines['top'].set_visible(False)            ax.spines['right'].set_visible(False)    plt.tight_layout()    plt.show()# Callplot_contrast_grid_combined(    band_cols=["rel_delta_ff", "rel_alpha_ff"],    band_labels=["delta", "alpha"])# plt.savefig("/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/figures/supp_fig/alpha_theta.png", dpi=300, bbox_inches='tight')#%% PSD across clustersimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltpath_table = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'path_eeg = '/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv'table = pd.read_csv(path_table)dimensions = ['Bizarreness', 'Fluidity', 'Spontaneity','Wake level']behavior = {"eyes-open", "trial-too-short","intentional","moving"}sub_behav = table[table['Behavior'].isin(behavior)].index.tolist()subs_no_content = table.index[table[dimensions[:-1]].sum(axis=1) == 0].tolist()subs_nan = table.index[table[dimensions].isnull().any(axis=1)].tolist()subs_to_excl = list(set(sorted(sub_behav + subs_no_content + subs_nan)))table_subs_excluded = table.drop(subs_to_excl)scores = table[dimensions].drop(subs_to_excl)eeg = pd.read_csv(path_eeg)# Parameterschans = "roi" # "allchans", "midline"idx_fooof = "" # "" "_ff"midchans = ["Fz", "CPz", 'Oz']stages = ['W', '1', '2', 'MSE']stage_colors = {'W': 'red','MSE': 'purple','1': 'lightblue','2': 'darkblue'}clusters = [1, 2, 3, 4]cluster_colors = {1: '#ff5733',2: '#ffcc00',3: '#3366ff',4: '#cc33ff'}# Convert psd_datadef psd_to_array(psd_str):    psd_str = psd_str.replace('[', '').replace(']', '').replace('\n', '')    return np.array([float(val) for val in psd_str.split()])eeg[f'psd_data{idx_fooof}'] = eeg[f'psd_data{idx_fooof}'].apply(psd_to_array)roi_names = ['Frontal', 'Central', 'Occipital']frontal_channels = ['Fp1', 'Fp2', 'Fpz', 'AF3', 'AF4', 'AFz', 'F3', 'F4', 'Fz', 'F1', 'F2']central_channels = ['FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C1', 'C2', 'C3', 'C4', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4']occipital_channels = ['P3', 'P1', 'Pz', 'P2', 'P4', 'PO3', 'PO1', 'POz', 'PO2', 'PO4', 'Oz', 'O1', 'O2']roi_channels = [frontal_channels, central_channels, occipital_channels]    # Set font sizestitle_fontsize = 24label_fontsize = 24tick_fontsize = 24fig, axes = plt.subplots(1, len(roi_names), figsize=(12, 6 * len(roi_names)), sharex=True)# Loop through ROIs and plot on each subplotfor idx, (roi_name, channels) in enumerate(zip(roi_names, roi_channels)):    ax = axes[idx]        # Get the psd_data for all channels in the current ROI and average them    roi_psds = []    for subject in eeg['subject'].unique():        subject_channels_psd = eeg[            (eeg['subject'] == subject) &             (eeg['channel'].isin(channels))        ][f'psd_data{idx_fooof}'].values        avg_psd = np.mean(subject_channels_psd, axis=0)        roi_psds.append(avg_psd)        # Create DataFrame with averaged ROI data    eeg_roi_psd = pd.DataFrame({        'subject': eeg['subject'].unique(),        'roi_psd_data': roi_psds    })        # Merge with gigatable    table['subject_key'] = table['Subject'] + '_' + table['Probe'] + '_' + table['Group']    merged_df = pd.merge(table, eeg_roi_psd[['subject', 'roi_psd_data']],                         left_on='subject_key', right_on='subject', how='left')    merged_df = merged_df.drop(columns=['subject'])        # Average by cluster    grouped_data = {}    for cluster in clusters:        group = merged_df[merged_df['Cluster'] == cluster]        avg_psd = np.mean(np.vstack(group['roi_psd_data'].dropna().values), axis=0)                if idx_fooof == "":            avg_psd = 10 * np.log10(avg_psd + 1e-12)                grouped_data[cluster] = avg_psd        freqs = np.linspace(0, 40, len(avg_psd))    # Plot curves    for cluster, psd_data in grouped_data.items():        ax.plot(freqs, psd_data,                label=f'Cluster {cluster} (n={len(merged_df[merged_df["Cluster"] == cluster])})',                color=cluster_colors[cluster],                linewidth=2.5,                zorder=10 if cluster == 0 else 1)    # Axis formatting    ax.set_title(f'{roi_name}', fontsize=title_fontsize)    ax.tick_params(labelsize=tick_fontsize)    if idx == 0:        ax.set_ylabel('PSD (dB)', fontsize=label_fontsize)    else:        ax.set_ylabel('')        ax.set_yticklabels([])        ax.tick_params(axis='y', left=False)    ax.spines['top'].set_visible(False)    ax.spines['right'].set_visible(False)# Shared x-axis labelfig.text(0.5, 0.04, 'Frequency (Hz)', ha='center', fontsize=label_fontsize)# Layoutplt.subplots_adjust(wspace=0.1)plt.tight_layout(rect=[0, 0.05, 1, 1])plt.show() # plt.savefig("/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/figures/supp_fig/psd.png", dpi=300, bbox_inches='tight')#%% PSD across sleep stagesimport pandas as pdimport numpy as npimport os, mneimport matplotlib.pyplot as pltpath_table = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'path_eeg = '/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv'# Load tablegigatable = pd.read_csv(path_table, sep=None) eeg = pd.read_csv(path_eeg)# Parametersidx_fooof = "" midchans = ["Fz", "CPz", 'Oz']stages = ['W', '1', '2', 'MSE']stage_colors = {'W': 'red','MSE': 'purple','1': 'lightblue','2': 'darkblue'}# Convert psd_datadef psd_to_array(psd_str):    psd_str = psd_str.replace('[', '').replace(']', '').replace('\n', '')    return np.array([float(val) for val in psd_str.split()])eeg[f'psd_data{idx_fooof}'] = eeg[f'psd_data{idx_fooof}'].apply(psd_to_array)    # Calculate the average psd across channels for each subjecteeg_avg_psd = eeg.groupby('subject')[f'psd_data{idx_fooof}'].apply(lambda x: np.mean(np.vstack(x), axis=0)).reset_index()eeg_avg_psd.rename(columns={f'psd_data{idx_fooof}': 'avg_psd_data'}, inplace=True)# Add the averaged psd data to eegeeg = eeg.merge(eeg_avg_psd[['subject', 'avg_psd_data']], on='subject', how='left')eeg = eeg[['subject', 'avg_psd_data']]eeg = eeg.drop_duplicates(subset='subject', keep='first')# Merge eeg and gigatablegigatable['subject_key'] = gigatable['Subject'] + '_' + gigatable['Probe'] + '_' + gigatable['Group']merged_df = pd.merge(gigatable, eeg[['subject','avg_psd_data']], left_on='subject_key', right_on='subject', how='left')merged_df = merged_df.drop(columns=['subject'])# Gather 1 and MSE# Replace 'MSE' with '1' in stage labelsmerged_df['Stage-at-probe'] = merged_df['Stage-at-probe'].replace('MSE', '1')# Define the stagesstages = ['W', '1', '2']# Prepare grouped datagrouped_data = {}for stage in stages:    group = merged_df[merged_df['Stage-at-probe'] == stage]['avg_psd_data'].dropna()    if not group.empty:        avg_psd = np.mean(np.vstack(group.values), axis=0)        if idx_fooof == "":            avg_psd = 10 * np.log10(avg_psd + 1e-12)        grouped_data[stage] = avg_psd# Set font sizestitle_fontsize = 22label_fontsize = 22tick_fontsize = 22# Create the figureplt.figure(figsize=(12, 6))freqs = np.linspace(0, 40, len(next(iter(grouped_data.values()))))# Define mappingstage_labels = {'W': 'Wake', '1': 'N1', '2': 'N2'}# Plot PSDs by stagefor stage in ['W', '1', '2']:  # Ordered plotting    psd_data = grouped_data[stage]    plt.plot(        freqs, psd_data,        label=stage_labels[stage],        color=stage_colors[stage],        linewidth=2.5    )# Style the axesax = plt.gca()ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)ax.tick_params(labelsize=tick_fontsize)# Add labelsplt.xlabel('Frequency (Hz)', fontsize=label_fontsize)plt.ylabel('PSD (dB)', fontsize=label_fontsize)# Add legendplt.legend(fontsize=tick_fontsize,frameon=False)# Final layoutplt.tight_layout()plt.show()# plt.savefig("/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/figures/supp_fig/psd_sleep.png", dpi=300, bbox_inches='tight')