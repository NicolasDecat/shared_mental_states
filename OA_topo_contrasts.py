#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Sep 19 14:07:59 2023NapNest: Topographical pairwise comparisons across clusters@author: nico"""#%% Paths & Packagesimport pandas as pdimport numpy as npimport pickleimport os, mneimport matplotlib.pyplot as pltimport statsmodels.formula.api as smffrom mpl_toolkits.axes_grid1 import make_axes_locatablefrom itertools import combinations from scipy.spatial import Delaunay# Parameters datalen_seg_sec = 10correction = "corrected"eeg_feat_col = ['rel_delta_ff','rel_theta_ff','rel_alpha_ff','rel_beta_ff','rel_gamma_ff','ratio_delta_alpha',                'Kolmogorov','Permutation_Entropy_theta','Sample_Entropy',                'slope','offset','total_power']# Define paths path_data = '/Volumes/disk-nico/napnest/data/' path_table = '/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/gigatable_4c.csv'# path_eeg = "/Volumes/disk-nico/napnest/napnest_eeg_feat/eeg_features_10-0.csv"path_eeg = "/Users/nicolas.decat/Downloads/eeg_features_10-0.csv"path_save_topo =  f"/Users/nicolas.decat/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/projects/NapNest/subjective_dimensions/topos/topos_4c_{len_seg_sec}s/all_combi_stg_corr/new_model_permcorr_mainfigs/"# Import table + eegtable = pd.read_csv(path_table, index_col=0, sep=None)eeg = pd.read_csv(path_eeg, index_col=0)eeg = eeg.drop(columns=['psd_data', 'psd_data_ff'])# Merge table and eegeeg[['Subject', 'Probe', 'Group']] = eeg['subject'].str.split('_', expand=True)eeg = eeg.merge(table,                 left_on=['Subject', 'Probe', 'Group'],                 right_on=['Subject', 'Probe', 'Group'],                 how='left')# Add ratio alpha theta, total spectral power and inter-channel varianceeeg['ratio_delta_alpha'] = eeg['abs_delta'] / eeg['abs_alpha']eeg['total_power'] = eeg[['abs_delta', 'abs_theta', 'abs_alpha', 'abs_beta', 'abs_gamma']].sum(axis=1)subject_variance = (eeg.groupby('subject')[['rel_delta_ff', 'rel_theta_ff', 'rel_alpha_ff', 'rel_beta_ff', 'rel_gamma_ff']].var().mean(axis=1))eeg['chan_var'] = eeg['subject'].map(subject_variance)# Only keep Stage-at-probe = Wake# eeg = eeg[eeg['Stage-at-probe'].isin(['1','MSE'])]# Replace 'N1' and 'MSE' with 'N1MSE' in the 'Stage-at-probe' columneeg['Stage-at-probe'] = eeg['Stage-at-probe'].replace({'1': 'N1MSE', 'MSE': 'N1MSE'})# Remove N2 eeg = eeg[eeg['Stage-at-probe'] != '2']# Import random raw data to get channel positionsub = 'VD002'# path_vhdr = path_data + '{}/{}.vhdr'.format(sub,sub)path_vhdr = "/Users/nicolas.decat/Downloads/VD002/VD002.vhdr"raw = mne.io.read_raw_brainvision(vhdr_fname=path_vhdr)raw.drop_channels(["EMG chin", "EMG arm", "ECG", "IO"])channels = np.asarray(raw.ch_names)# %% Functions Cluster Permdef prepare_neighbours_from_layout(info, ch_type='eeg'):    """    Create a neighbours structure similar to FieldTrip's ft_prepare_neighbours    using Delaunay triangulation.    Parameters    ----------    info : MNE Info object        contains sensor locations..    ch_type : str, optional        Type of channels to consider (e.g., 'eeg'). The default is 'eeg'.    Returns    -------    neighbours_list : A list of dicts.        With each channel's label and its neighbours.    """    # Get the 2D positions of the channels from info    pos = []    labels = []    for ch in info['chs']:        # Filter by channel type if needed, e.g., check ch['kind'] or use mne.pick_types.        # Here we simply assume that the info is for the ch_type of interest.        if 'loc' in ch:            # Use the first two coordinates from the sensor location as 2D projection            pos.append(ch['loc'][:2])            labels.append(ch['ch_name'])        pos = np.array(pos)        # Perform Delaunay triangulation    tri = Delaunay(pos)        # Create a dictionary where each channel has a set of neighbours    neighbours = {label: set() for label in labels}        # For each simplex (triangle) in the triangulation, add edges between sensors    for simplex in tri.simplices:        for i in range(3):            ch_i = labels[simplex[i]]            for j in range(i + 1, 3):                ch_j = labels[simplex[j]]                neighbours[ch_i].add(ch_j)                neighbours[ch_j].add(ch_i)        # Format the neighbours info as a list of dictionaries (similar to FieldTrip's structure)    neighbours_list = []    for label in labels:        neighbours_list.append({            'label': label,            'neighblabel': list(neighbours[label])        })        return neighbours_list# ============================================================================# 1. Run mixed-effects model for a given channel (unchanged)def run_mixedlm(data, channel, interest, model):    """    Run a mixed linear model for a given channel    Parameters    ----------    data : Pandas DataFrame        DataFrame containing the values to run LME.    channel : List        List of channels in the correct order for later plotting.    interest : str        Name of effect of interest to collect p and t values.    model : str        Models to run LME.    Returns    -------    p_values_oi : list        p values of interest.    t_values_oi : list        t values of interest.    """        ##### Use LM (sub as random)        subdf = data.loc[data.channel == channel].dropna()    md = smf.mixedlm(        model, subdf, groups=subdf['sub']        )    mdf = md.fit()    p_values_oi = mdf.pvalues[interest]    t_values_oi = mdf.tvalues[interest]    return (p_values_oi, t_values_oi)    # ============================================================================# 2. Updated helper function: Cluster significant channels using spatial neighbours,#    ensuring only candidate channels (uncorrected p < clus_alpha) are included.def cluster_significant_channels(        channels,         pvals,         tvals,         neighbours,         clus_alpha,         min_cluster_size,         sign='pos'        ):    """    Create clusters of significant channels based solely on candidate channels.    1) Select candidate channels (p < clus_alpha, correct sign).    2) Seed one‐channel clusters for each candidate.    3) Iteratively merge any two clusters if any channel in A is a neighbour       of any channel in B (using the supplied neighbours map).    4) Discard clusters smaller than min_cluster_size.    Parameters    ----------    channels : List        List of channel labels.    pvals : List        List of p-values for each channel.    tvals : List        List of t-values for each channel.    neighbours : List of dicts        List of dicts. Each dict has keys 'label' (channel label)        and 'neighblabel' (list of neighbouring channel labels).    clus_alpha : float        The uncorrected p-value threshold.    min_cluster_size : float        Minimum number of channels required for a valid cluster.    sign : str, optional        'pos' for positive effects, 'neg' for negative. The default is 'pos'.    Returns    -------    clusters : List        A list of clusters. Each cluster is a dict with keys:        'labels'  : a set of channel labels that are candidates and belong to the cluster,        'tstats'  : a list of t-values for those channels,        'neighbs' : the union of candidate neighbour labels for all channels in the cluster.    """    # 1) Build candidate set    candidate_set = {        channels[i]        for i in range(len(channels))        if (pvals[i] < clus_alpha)           and ((sign=='pos' and tvals[i]>0) or (sign=='neg' and tvals[i]<0))    }    # Quick neighbour lookup: channel -> set of its neighbours (intersected with candidates)    neigh_map = {        n['label']: set(n['neighblabel']).intersection(candidate_set)        for n in neighbours        if n['label'] in candidate_set    }    # 2) Seed initial one‐channel clusters    clusters = []    for ch in candidate_set:        clusters.append({            'labels': {ch},            'tstats': [tvals[np.where(channels == ch)[0][0]]]        })        # 3) Iteratively merge any two clusters that touch    merged = True    while merged:        merged = False        new_clusters = []        used = [False]*len(clusters)        for i, ci in enumerate(clusters):            if used[i]:                continue            # try to absorb any later cluster that’s adjacent            for j in range(i+1, len(clusters)):                if used[j]:                    continue                cj = clusters[j]                # check adjacency: any channel in ci is neighbour of any in cj?                if any(                    (label in neigh_map and neigh_map[label] & cj['labels'])                    for label in ci['labels']                ) or any(                    (label in neigh_map and neigh_map[label] & ci['labels'])                    for label in cj['labels']                ):                    # fuse j into i                    ci['labels'] |= cj['labels']                    ci['tstats']  += cj['tstats']                    used[j] = True                    merged = True            new_clusters.append(ci)        clusters = new_clusters        # 4) Filter by minimum cluster size    clusters = [c for c in clusters if len(c['labels']) >= min_cluster_size]    return clusters# ============================================================================# 3. Permutation procedure with clustering using neighbour informationdef permute_and_cluster(        data,         model,        interest,         to_permute,        num_permutations,         neighbours,         clus_alpha,         min_cluster_size,        channels        ):    """    Compute original channel p-values and t-values, build clusters based on     neighbours, and then generate a null distribution via permutation clustering.    Parameters    ----------    data : Pandas DataFrame        DataFrame containing the data.    model : str        The model to run linear mixed models.    interest : str        The effect of interest (e.g., 'n_session:C(difficulty)[T.HARD]').    num_permutations : int        Number of permutations.    neighbours : List of Dict        Neighbours structure (list of dicts as produced by e.g., prepare_neighbours_from_layout).    clus_alpha : float        Uncorrected p-value threshold (e.g., 0.05).    min_cluster_size : int        Minimum number of channels per cluster..    channels : list of str of np.array of str        containes channels in the correct order for laters topographies    Returns    -------    clusters_pos : List (to complete)        Clusters from the real (non-permuted) data (for positive effects)..    clusters_neg : List (to complete)        Clusters from the real (non-permuted) data (for negative effects)..    perm_stats_pos : List        Lists of maximum cluster stats from each permutation.    perm_stats_neg : List        Lists of maximum cluster stats from each permutation.    original_pvals : List        Original channel-level statistics.    original_tvals : List        Original channel-level statistics.    channels : List        Original channel-level statistics.    """    original_pvals = []    original_tvals = []    for chan in channels:        p, t = run_mixedlm(data, chan, interest, model)        original_pvals.append(p)        original_tvals.append(t)        # Form clusters separately for positive and negative effects.    clusters_pos = cluster_significant_channels(        channels,         original_pvals,         original_tvals,        neighbours,         clus_alpha,         min_cluster_size,         sign='pos'        )    clusters_neg = cluster_significant_channels(        channels,         original_pvals,         original_tvals,        neighbours,         clus_alpha,         min_cluster_size,         sign='neg'        )        perm_stats_pos = []  # one value per permutation: maximum cluster t-sum (for positive clusters)    perm_stats_neg = []  # one value per permutation: minimum (most negative) cluster t-sum (for negative clusters)        for _ in range(num_permutations):        shuffled_data = data.copy()        shuffled_data[to_permute] = np.random.permutation(shuffled_data[to_permute].values)        perm_pvals = []        perm_tvals = []        for chan in channels:            p, t = run_mixedlm(shuffled_data, chan, interest, model)            perm_pvals.append(p)            perm_tvals.append(t)                perm_clusters_pos = cluster_significant_channels(            channels,             perm_pvals,             perm_tvals,            neighbours,             clus_alpha,             min_cluster_size,             sign='pos')        perm_clusters_neg = cluster_significant_channels(            channels,            perm_pvals,             perm_tvals,            neighbours,             clus_alpha,             min_cluster_size,             sign='neg'            )                if perm_clusters_pos:            perm_stat_pos = max(sum(clust['tstats']) for clust in perm_clusters_pos)        else:            perm_stat_pos = 0        perm_stats_pos.append(perm_stat_pos)                if perm_clusters_neg:            perm_stat_neg = min(sum(clust['tstats']) for clust in perm_clusters_neg)        else:            perm_stat_neg = 0        perm_stats_neg.append(perm_stat_neg)        return clusters_pos, clusters_neg, perm_stats_pos, perm_stats_neg, original_pvals, original_tvals, channels# ============================================================================# 4. Determine which real clusters are significant via permutation comparisondef identify_significant_clusters(        clusters_pos,        clusters_neg,         perm_stats_pos,         perm_stats_neg,         montecarlo_alpha,         num_permutations        ):    """    Compare each original cluster statistic against its permutation distribution and return    those clusters that are significant.    Parameters    ----------    clusters_pos : List (to complete)        Clusters from the real (non-permuted) data (for positive effects)..    clusters_neg : List (to complete)        Clusters from the real (non-permuted) data (for negative effects)..    perm_stats_pos : List        Lists of maximum cluster stats from each permutation.    perm_stats_neg : List        Lists of maximum cluster stats from each permutation.    montecarlo_alpha : TYPE        DESCRIPTION.    num_permutations : TYPE        DESCRIPTION.    Returns    -------    significant_clusters : A list of tuple        (sign, cluster_labels, cluster_stat, p_value)        where sign is 'pos' or 'neg'.    """        significant_clusters = []    for clust in clusters_pos:        stat = sum(clust['tstats'])        p_value = (np.sum(np.array(perm_stats_pos) >= stat) + 1) / (num_permutations + 1)        if p_value < montecarlo_alpha:            significant_clusters.append(('pos', clust['labels'], stat, p_value))        for clust in clusters_neg:        stat = sum(clust['tstats'])        p_value = (np.sum(np.array(perm_stats_neg) <= stat) + 1) / (num_permutations + 1)        if p_value < montecarlo_alpha:            significant_clusters.append(('neg', clust['labels'], stat, p_value))        return significant_clusters# ============================================================================# 5. Visualization functiondef visualize_clusters(tvals, channels, significant_mask, info, savepath):    """    Visualize significant clusters using topomap.    Parameters    ----------    tvals : TYPE        DESCRIPTION.    channels : TYPE        DESCRIPTION.    significant_mask : TYPE        DESCRIPTION.    info : TYPE        DESCRIPTION.    savepath : TYPE        DESCRIPTION.    Returns    -------    None.    """    fig, ax = plt.subplots(figsize=(5, 5))    im, cm = mne.viz.plot_topomap(        data=np.array(tvals),        pos=info,          mask=significant_mask,        axes=ax,        show=False,        contours=2,        mask_params = dict(            marker='o',             markerfacecolor='w',             markeredgecolor='k',            linewidth=0,               markersize=12        ),        # cmap="viridis",        # cmap = "RdBu_r",        cmap = "RdBu_r",        vlim = (-3, 3)    )    divider = make_axes_locatable(ax)    cax = divider.append_axes("right", size="5%", pad=0.25)    cax.set_title("t-values", fontsize=14)    cb = fig.colorbar(im, cax=cax)    cb.ax.tick_params(labelsize=14)    fig.colorbar(im, cax=cax)        fig.suptitle(f"{metric} || C{cluster2} - C{cluster1} ({correction})", fontweight = "bold")    plt.savefig(savepath, dpi = 300)    plt.show()    # %% Arthur Run Cluster Perminfo = raw.info  # or info from epochsneighbours = prepare_neighbours_from_layout(info, ch_type='eeg')    #%% Run Cluster Perm # Get dimension and EEG metric namesmetrics_eeg = eeg_feat_coldimension = 'Cluster'print(f"Fitting model for {dimension}")# Generate combinations of clustersclusters = [1, 2, 3, 4]cluster_combinations = list(combinations(clusters, 2))clus_alpha = 0.05        # uncorrected threshold for candidate electrodesmontecarlo_alpha = 0.05  # threshold for permutation cluster-level testnum_permutations = 100   # adjust as neededmin_cluster_size = 2     # keep clusters with at least 2 channels# If wanna target spe combinationscluster_combinations = [(1,2),(3,2),(4,2)]# cluster_combinations = [pair for pair in cluster_combinations if 3 not in pair]metrics_eeg = ['Sample_Entropy']# Iterate over each combinationfor cluster_pair in cluster_combinations:        cluster1, cluster2 = cluster_pair        interest = f'C(Cluster, Treatment({cluster1}))[T.{cluster2}.0]'        print(f"Fitting model for clusters {cluster1} vs {cluster2}")        # Loop through EEG metrics    for metric in metrics_eeg :                subdf = eeg[            ['subject', dimension, 'channel', metric,'Stage-at-probe']            ].dropna()          subdf.rename(columns={'Stage-at-probe': 'Stage'}, inplace=True)                subdf['sub'] = subdf['subject'].str[:5]        # z-score the dimension scores per channel        subdf[metric] = subdf[metric].apply(lambda x: (x - subdf[metric].mean()) / subdf[metric].std())        # Only get values from the right combination of clusters        subdf = subdf[(subdf['Cluster'] == cluster1) | (subdf['Cluster'] == cluster2)]                model = f"{metric} ~ C(Cluster, Treatment({cluster1})) + Stage"        to_permute = "Cluster"                clusters_pos, clusters_neg, perm_stats_pos, perm_stats_neg, orig_pvals, orig_tvals, channels = permute_and_cluster(            subdf,            model,             interest,            to_permute,            num_permutations,            neighbours,                 clus_alpha,            min_cluster_size,            channels            )                significant_clusters = identify_significant_clusters(            clusters_pos,             clusters_neg,             perm_stats_pos,             perm_stats_neg,             montecarlo_alpha,            num_permutations            )                #  ➤ Save everything for later use (before you do the plotting!)        out = {            'clusters_pos':            clusters_pos,            'clusters_neg':            clusters_neg,            'perm_stats_pos':          perm_stats_pos,            'perm_stats_neg':          perm_stats_neg,            'orig_pvals':              orig_pvals,            'orig_tvals':              orig_tvals,            'channels':                channels,            'significant_clusters':    significant_clusters            }        save_fname = os.path.join(            path_save_topo,            f"cluster_perm_{metric}_C{cluster2}-C{cluster1}_{len_seg_sec}s_{num_permutations}.pkl"            )        os.makedirs(os.path.dirname(save_fname), exist_ok=True)        with open(save_fname, 'wb') as fp:            pickle.dump(out, fp)        print(f"Saved permutation‐cluster results to {save_fname}")                 # Build the mask from these merged clusters        significant_mask = np.zeros(len(channels), dtype=bool)        for sign, clust_labels, stat, pval in significant_clusters:            for ch in clust_labels:                idx = np.where(channels == ch)[0][0]                significant_mask[idx] = True            # Visualize using the original t-values        savepath = os.path.join(            path_save_topo,             f"{metric}_C{cluster2}-C{cluster1}_{len_seg_sec}s_{correction}.png"            )        visualize_clusters(            orig_tvals, channels, significant_mask, info, savepath            )    